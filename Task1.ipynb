{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a359d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from urllib.request import Request, urlopen, urlretrieve\n",
    "\n",
    "from __future__ import print_function\n",
    "import multiprocessing\n",
    "import os\n",
    "import datetime\n",
    "import zipfile\n",
    "import tempfile\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import io\n",
    "import time\n",
    "\n",
    "EDGAR_PREFIX = \"https://www.sec.gov/Archives/\"\n",
    "SEP = \"|\"\n",
    "IS_PY3 = sys.version_info[0] >= 3\n",
    "REQUEST_BUDGET_MS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f7212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"download_master\"\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e64a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _quarterly_idx_list(since_year=1995):\n",
    "    \"\"\"\n",
    "    Generate the list of quarterly zip files archived in EDGAR\n",
    "    since 1995 until this previous quarter\n",
    "    \"\"\"\n",
    "    logging.debug(\"downloading files since %s\" % since_year)\n",
    "    years = range(since_year, 2022)\n",
    "    quarters = [\"QTR1\", \"QTR2\", \"QTR3\", \"QTR4\"]\n",
    "    history = list((y, q) for y in years for q in quarters)\n",
    "\n",
    "    return [\n",
    "        (\n",
    "            EDGAR_PREFIX + \"edgar/full-index/%s/%s/master.zip\" % (x[0], x[1]),\n",
    "            \"%s-%s.tsv\" % (x[0], x[1]),\n",
    "        )\n",
    "        for x in history\n",
    "    ]\n",
    "\n",
    "\n",
    "def _append_line(line):\n",
    "    chunks = line.split(SEP)\n",
    "    if chunks[2]==\"8-K\":\n",
    "        return line\n",
    "\n",
    "def _is_8k(line):\n",
    "    chunks = line.split(\"|\")\n",
    "    if chunks[2]==\"8-K\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _skip_header(f):\n",
    "    for x in range(0, 11):\n",
    "        f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14763fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _url_get(url, user_agent):\n",
    "    content = None\n",
    "    if IS_PY3:\n",
    "        # python 3\n",
    "        import urllib.request\n",
    "        hdr = { 'User-Agent' : user_agent }\n",
    "        req = urllib.request.Request(url, headers=hdr)\n",
    "        content =urllib.request.urlopen(req).read()\n",
    "    else:\n",
    "        # python 2\n",
    "        import urllib2\n",
    "\n",
    "        content = urllib2.urlopen(url).read()\n",
    "    return content\n",
    "\n",
    "def _download(file, dest, skip_file, user_agent):\n",
    "    \"\"\"\n",
    "    Download an idx archive from EDGAR\n",
    "    This will read idx files and unzip\n",
    "    archives + read the master.idx file inside\n",
    "    when skip_file is True, it will skip the file if it's already present.\n",
    "    \"\"\"\n",
    "    if not dest.endswith(\"/\"):\n",
    "        dest = \"%s/\" % dest\n",
    "\n",
    "    url = file[0]\n",
    "    dest_name = file[1]\n",
    "    if skip_file and os.path.exists(dest+dest_name):\n",
    "        logging.info(\"> Skipping %s\" % (dest_name))\n",
    "        return\n",
    "\n",
    "    if url.endswith(\"zip\"):\n",
    "        with tempfile.TemporaryFile(mode=\"w+b\") as tmp:\n",
    "            tmp.write(_url_get(url, user_agent))\n",
    "            with zipfile.ZipFile(tmp).open(\"master.idx\") as z:\n",
    "                with io.open(dest + dest_name, \"w+\", encoding=\"utf-8\") as idxfile:\n",
    "                    _skip_header(z)\n",
    "                    lines = z.read()\n",
    "                    if IS_PY3:\n",
    "                        lines = lines.decode(\"latin-1\")\n",
    "                    new_lines = lines.splitlines()\n",
    "                    latest_lines = \"\"\n",
    "                    for line_new in new_lines:\n",
    "                        if _is_8k(line_new):\n",
    "                            latest_lines+=line_new + \"\\n\"\n",
    "\n",
    "                    lines = map(\n",
    "                        lambda line: _append_line(line), latest_lines.splitlines() \n",
    "                    )\n",
    "                    idxfile.write(\"\\n\".join(lines)+\"\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise logging.error(\"python-edgar only supports zipped index files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2cc3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_millis():\n",
    "    return round(time.time() * 1000)\n",
    "\n",
    "def download_index(dest, since_year, user_agent, skip_all_present_except_last=False):\n",
    "    \"\"\"\n",
    "    Convenient method to download all files at once\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest)\n",
    "\n",
    "    tasks = _quarterly_idx_list(since_year)\n",
    "    logging.info(\"%d index files to retrieve\", len(tasks))\n",
    "    last_download_at = _get_millis()\n",
    "    for i, file in enumerate(tasks):\n",
    "        skip_file = skip_all_present_except_last\n",
    "        if i == 0:\n",
    "            # First one should always be re-downloaded\n",
    "            skip_file = False\n",
    "        # naive: 200ms or 5QPS serialized\n",
    "        start = _get_millis()\n",
    "        _download(file, dest, skip_file, user_agent)\n",
    "        elapsed = _get_millis() - start\n",
    "        if elapsed < REQUEST_BUDGET_MS:\n",
    "            sleep_for = REQUEST_BUDGET_MS-elapsed\n",
    "            logging.info(\"sleeping for %dms because we are going too fast (previous request took %dms\", sleep_for, elapsed)\n",
    "            time.sleep(sleep_for/1000)\n",
    "        last_download_at = _get_millis()\n",
    "\n",
    "\n",
    "    logging.info(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a5df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Please check your user agent and make changes accordinlgy\n",
    "download_index(\"download_master\", 1995, \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38eb3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"8K_filing\"\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c5d09d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk.corpus import stopwords\n",
    "import urllib\n",
    "from urllib.request import Request, urlopen\n",
    "import csv\n",
    "import unicodedata\n",
    "\n",
    "directory = \"download_master\"\n",
    "SEP = \"|\"\n",
    "EDGAR_PREFIX = \"https://www.sec.gov/Archives/\"\n",
    "start = '<TYPE>8-K'\n",
    "end = '</DOCUMENT>'\n",
    "result=\"\"\n",
    "\n",
    "\n",
    "def get_url(line):\n",
    "    chunks = line.split(SEP)\n",
    "    return chunks[-1]\n",
    "\n",
    "def get_cik(line):\n",
    "    chunks = line.split(SEP)\n",
    "    return chunks[0]\n",
    "\n",
    "\n",
    "def get_date(line):\n",
    "    chunks = line.split(SEP)\n",
    "    return chunks[3]\n",
    "\n",
    "def get_number(line):\n",
    "    chunks = line.split(\"/\")\n",
    "    return chunks[-1]\n",
    "\n",
    "stopwords = ['copyright','html','webfilings','zip code', 'pagebreak',\n",
    "             'table','body','value','per','securities','exchange','comission','telephone','number',\n",
    "             'code', 'page','xbrl','begin','abn amro','abnormal',\n",
    "             'aoci','anne','anda',\n",
    "             'bbls','bcfe','asic','asus','blvd',\n",
    "             'btus', \"date\"\n",
    "             'cceeff','cdos','cede','cmsa','conway',\n",
    "             'dana','wyeth','wyoming','xannual','xerox','xiii','xindicate',\n",
    "             'xvii','xviii',\n",
    "             'tdrs','form', 'january', 'february', 'march',\n",
    "             'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december',\n",
    "             ' january', ' february', ' march',\n",
    "             ' april', ' may', ' june', ' july', ' august', ' september', ' october', ' november', ' december',\n",
    "             '__________________________________________ '\n",
    "            '__________ ']\n",
    "\n",
    "def find_between(s,start,end):\n",
    "    \"\"\"\n",
    "    Find the text between  <TYPE>8-K and </DOCUMENT>\n",
    "    \"\"\"\n",
    "    return (s.split(start))[1].split(end)[0]\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "#function for removing all the scrub words\n",
    "def scrub_words(text):\n",
    "    \n",
    "    #Replace \\xao characters in text\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    \n",
    "    #Replace non ascii / not words and digits\n",
    "    text = re.sub(\"(\\\\W|\\\\d)\",' ',text)\n",
    "     \n",
    "    #Replace new line characters and following text until space\n",
    "    text = re.sub('\\n(\\w*?)[\\s]', '', text)\n",
    "     \n",
    "    #Remove extra spaces from the text\n",
    "    text = re.sub(\"\\s+\", ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_head(text):\n",
    "    \"\"\"\n",
    "    Removes the starting part of the text which is not relevant\n",
    "    \"\"\"\n",
    "    text = text.replace(\" html PUBLIC W C DTD HTML Transitional EN http www w org TR html loose dtd Document created using Wdesk Copyright Workiva Document \", \"\")\n",
    "    \n",
    "    text = text.replace(\" SECURITIES AND EXCHANGE COMMISSION Washington D C FORM K CURRENT REPORT Pursuant to Section or d of the Securities Exchange Act of Date of Report Date of earliest event reported \", \"\")\n",
    "    text = text.replace(\" SECURITIES AND EXCHANGE COMMISSION Washington D C Form K CURRENT REPORT Pursuant to Section or d of the Securities Exchange Act of Date of Report Date of earliest event reported \", \"\")\n",
    "    text = text.replace(\" SECURITIES AND EXCHANGE COMMISSION Washington D C ________________ FORM K CURRENT REPORT PURSUANT TO SECTION OR d OF THE SECURITIES EXCHANGE ACT OF Date of report date of earliest event reported \", \"\")\n",
    "    text = text.replace(\"commission washington current report pursuant section date report date earliest event reported\", \"\")\n",
    "    text = text.replace(\" commission washington current report pursuant section date report date earliest event reported\",\"\")\n",
    "    text = text.replace(\"commission washington current report pursuant section date report\",\"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    \"\"\"\n",
    "    Removes all the words which are less than 4 characters and which are there in stopwords. For removing words less than 3 \n",
    "    characters instead of 4 characters use: new_string = ' '.join([w for w in old_string.split() if len(w)>2])\n",
    "    \"\"\"\n",
    "    remove = re.sub(r'\\b\\w{1,3}\\b', '', raw_review) #removing all words less than 4 characters \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", remove) \n",
    "    word = letters_only.lower().split()\n",
    "  \n",
    "    meaningful_words = [w for w in word if not w in stopwords]   ##removing all words in stopwords list\n",
    "    return( \" \".join(meaningful_words))\n",
    "\n",
    "\n",
    "# Creates a csv file to save the CIK no., Date of filing, name of the text file and Sentiment score\n",
    "temp_csv = open(\"csv_file.csv\", \"a+\", newline=\"\")\n",
    "csv_file = csv.writer(temp_csv)\n",
    "\n",
    "header=[\"CIK\", \"Date_of_filing\", \"Name_of_text_file\", \"Sentiment_Score\"]\n",
    "csv_file.writerow(header)\n",
    "\n",
    "## iterate over .tsv files in the directory\n",
    "# Might take upto 15-20 mins for the execution\n",
    "files = Path(directory).glob('*')\n",
    "count=0\n",
    "for file in files:\n",
    "    with open(file, \"r+\") as source:\n",
    "        lines = [line.replace(\"\\n\", \"\") for line in source]\n",
    "        random_line = random.sample(lines, 10)                  ## To choice 10 random lines from each .tsv file\n",
    "        \n",
    "        for i in range(0, 10):\n",
    "            \n",
    "            line = random_line[i]      \n",
    "            url_to_8K = EDGAR_PREFIX + get_url(line)\n",
    "            cik = get_cik(line)\n",
    "            date_of_filing = get_date(line)\n",
    "            unique_name = get_number(url_to_8K)\n",
    "            \n",
    "            ##Extracting the text from the url\n",
    "            req = Request(url_to_8K, headers={'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "            web_byte = urlopen(req).read()\n",
    "            web_byte = web_byte.decode(\"utf-8\")\n",
    "            result = find_between(web_byte,start,end)\n",
    "            result = re.sub(\"<.*?>\", ' ', result)\n",
    "            soup = bs(result,'lxml')\n",
    "            text =soup.find_all(text=True)\n",
    "            \n",
    "            ##Cleaning the text\n",
    "            blacklist = ['a','sequence','filename','description']\n",
    "            output =\"\"\n",
    "            for t in text:    \n",
    "                if t.parent.name not in blacklist:\n",
    "                    output += '{}'.format(t)\n",
    "            output = re.sub(\"<.*?>\", ' ', output)\n",
    "            output = remove_accented_chars(output)\n",
    "            output = scrub_words(output)\n",
    "            output = review_to_words(output)\n",
    "            output = remove_head(output)\n",
    "            \n",
    "            ##Saving details in the dataframe\n",
    "            details = [cik, date_of_filing, unique_name, 0]\n",
    "            csv_file.writerow(details)\n",
    "            \n",
    "            file = open(f\"8K_filing/{unique_name}\", \"a+\")\n",
    "            file.write(output)\n",
    "            \n",
    "            count=count+1\n",
    "\n",
    "            \n",
    "print(count)            \n",
    "temp_csv.close()          \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43744610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
